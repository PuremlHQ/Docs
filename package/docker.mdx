---
title: Docker
---

PureML uses Docker-py to create containers for the models.

## Installing Prerequisites

Before you start creating the container for your model, install the following clients on your machine. If not explicitly specified, use the latest available version of the specified components.

<Card
  title="Download docker"
  icon="link"
  href="https://docs.docker.com/engine/install/"
>
  Docker Engine is an open source containerization technology for building and
  containerizing your applications.
</Card>

To use GPU for the container, you need GPU to be installed in your system. Follow steps from below link and get ready for packaging:

<Card
  title="Download container toolkit"
  icon="link"
  href="https://docs.docker.com/config/containers/resource_constraints/#gpu"
>
  NVIDIA container toolkit is a wrapper around a docker container to provide
  access to system GPUs for the container.
</Card>

## Create your prediction function

Refer to [Prediction Section](../prediction/versioning) on to how to create a prediction function and add it to a version of a model.

## Creating Docker Container

Creating a docker container is way too easy to package your updated data than you think.

Create a `.env` file

<CodeGroup>
```properties .env
ORG_ID="<org_id>"
API_ID="<api_id>"
API_KEY="<api_key>"
```
</CodeGroup>

Once the env file is correctly loaded, you can proceed to create docker using `pureml`.

<Tabs>
  <Tab title="CPU">
    ```python
    import pureml

    env_path = "<env_path>"

    pureml.docker.create(label='churn classifier:dev:v5',
                         env_path=env_path,
                         access_token=access_token,)
    ```

  </Tab>
  <Tab title="GPU">
    If you are creating a container for the model that needs GPU container which needs GPU access, then you need to pass on additional parameters in `pureml.docker.create` command as shown below:

    ```python
    import pureml

    pureml.docker.create('flavia_tabnet_classifier:dev_2:v1',
                         org_id=org_id,
                         access_token=access_token,
                         runtime="nvidia",
                         gpu_ids=[0])
    ```
    <Info> PureML utilises NVIDIA container toolkit to provide access to system [GPUs in the container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/k8s/containers/container-toolkit).</Info>

    > In above code, only GPU with id 0 is used, if you want to use multiple GPUs available in your system then you should specify it with a separate id `gpu_ids=[0,1,2,..]`.

  </Tab>
</Tabs>

### Changing the port number

By default pureml host container on `0.0.0.0:8000`. If that port is already in use, the below command can be explicitly provided to change port to your custom port:

```python
import pureml

pureml.docker.create('flavia_tabnet_classifier:dev_2:v1',
                     org_id=org_id,
                     access_token=access_token,
                     port=<your port number>)
```

## Expected Output

If executed successfully, PureML returns the following message along with the URL where the model is hosted.

```

Taking the default predict.py file path: <path to predict.py>
Taking the requirements.txt file path: <path to requirements.txt>
FastAPI server files are created
Docker image is created
<Image: '<model name>:<model version>'>
Created Docker container
<Container: <container id>>
Prediction requests can be forwarded to 0.0.0.0:8000/predict

```

The model is hosted at `0.0.0.0:8000/predict`. Users can get predictions by sending get requests to the API.

## Sending Requests to the API

### Using Requests

The API call expects a parameter named `test_data` that contains data to be tested. This `data` will be passed on to the data parameter set in the `model_predict` function. PureML obtains the model specified in the `pureml.docker.create` command and sends it to themodel parameter of the `model_predict` function.

```python
import requests

params = {'test_data': <test data>}
headers = {'accept': 'application/json'}

response = requests.get('http://0.0.0.0:8000/predict',
                        params=params,
                        headers=headers)

predictions = response.json()
```

Predictions can be extracted from the json response.

### Using Curl

```Curl
curl -X 'GET'  'http://0.0.0.0:8000/predict?test_data=<test data>' -H 'accept: application/json'
```

## Accepted Input and Output types

The description of input and output data types is required to create the docker container. Here is the list of available data types:

### Input datatypes

The JSON data received by the API endpoint will convert the data into the required datatype specified by the user. For the image data type, API will load the received file though pillow library and convert it into a numpy ndarray before passing it into the prediction function.

- [Numpy array](https://numpy.org/doc/stable/): 'numpy ndarray'
- [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): 'pandas dataframe'
- [string](): 'text
- [Image](https://pillow.readthedocs.io/en/stable/reference/Image.html): 'image'

### Output datatypes

The model output will be converted into a JSON string and returned to the user upon an API call. Here are the supported output datatypes.

- [Numpy array](https://numpy.org/doc/stable/): 'numpy ndarray
- [Pandas dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): 'pandas dataframe
- [string](): 'text
